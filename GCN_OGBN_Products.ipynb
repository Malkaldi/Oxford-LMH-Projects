{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OGBN-Products"
      ],
      "metadata": {
        "id": "g_Vh9kosaChl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ogbn-products dataset is an undirected and unweighted graph, representing an Amazon product co-purchasing network. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. Node features are generated by extracting bag-of-words features from the product descriptions followed by a Principal Component Analysis to reduce the dimension to 100.\n",
        "\n",
        "The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels."
      ],
      "metadata": {
        "id": "8HFHJC-LaLyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vkP8pA1qBE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe55064-b307-485c-d586-0ae24955ac27"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6d22O6DqGSZ"
      },
      "source": [
        "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8hfxJ-qRg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc585d59-e368-4a42-a905-a8de1a7728cf"
      },
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.1+pt20cu118)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.17+pt20cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.16)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import DataLoader\n",
        "import numpy as np\n",
        "from torch_geometric.typing import SparseTensor\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3SkS1Mzcbe8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ibJ0ieoIwQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9afc85-c8bd-429b-f468-1c390b481153"
      },
      "source": [
        "dataset_name = 'ogbn-products'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "num_classes = dataset.num_classes\n",
        "num_features = data.num_features\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "\n",
        "del dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "nJ6I9prjdg57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4eb7867-0689-4b10-89a2-bc38db96b31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=2449029, x=[2449029, 100], y=[2449029, 1], adj_t=[2449029, 2449029, nnz=123718280])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is very big and if you try to run it as it is on colab, you may get an out of memory error.\n",
        "\n",
        "One solution is to use batching and train on subgraphs. Here, we will just make a smaller dataset so that we can train it in one go."
      ],
      "metadata": {
        "id": "L7QzFWvS2DvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to have edge indxes to make a subgraph. We can get those from the adjacency matrix.\n",
        "data.edge_index = torch.stack([data.adj_t.__dict__[\"storage\"]._row, data.adj_t.__dict__[\"storage\"]._col])\n",
        "\n",
        "# We will only use the first 100000 nodes.\n",
        "sub_nodes = 100000\n",
        "sub_graph = data.subgraph(torch.arange(sub_nodes))\n",
        "\n",
        "# Update the adjaceny matrix according to the new graph\n",
        "sub_graph.adj_t = SparseTensor(\n",
        "    row=sub_graph.edge_index[0],\n",
        "    col=sub_graph.edge_index[1],\n",
        "    sparse_sizes=None,\n",
        "    is_sorted=True,\n",
        "    trust_data=True,\n",
        ")\n",
        "\n",
        "sub_graph = sub_graph.to(device)\n",
        "\n",
        "sub_graph\n"
      ],
      "metadata": {
        "id": "q3Nrhnno6ylE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8b3555-daf3-4500-d916-bc40a305712f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=100000, x=[100000, 100], y=[100000, 1], adj_t=[100000, 100000, nnz=2818046], edge_index=[2, 2818046])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spilt data into train validation and test set\n",
        "split_sizes = [int(sub_nodes*0.8),int(sub_nodes*0.05),int(sub_nodes*0.15)]\n",
        "indices = torch.arange(sub_nodes)\n",
        "np.random.shuffle(indices.numpy())\n",
        "split_idx = {s:t for t,s in zip(torch.split(indices, split_sizes, dim=0), [\"train\", \"valid\", \"test\"])}\n",
        "split_idx"
      ],
      "metadata": {
        "id": "xJr8y1mG7xN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784378ca-c462-4ebb-f281-6ae41d029c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': tensor([65169, 75425, 35090,  ..., 86972, 54592, 46947]),\n",
              " 'valid': tensor([25857, 77680, 53759,  ..., 47115, 29915, 68283]),\n",
              " 'test': tensor([20662, 70490, 91664,  ..., 71758,  3518, 58299])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Feature Length of each node: {data.x.shape[1]}\")"
      ],
      "metadata": {
        "id": "WfHKG2kTcEOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd196ccb-7187-414d-e09c-bef2e0da35a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Length of each node: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgspXTYpNJLA"
      },
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = [\n",
        "            GCNConv(input_dim, hidden_dim),\n",
        "        ]\n",
        "\n",
        "        for _ in range(num_layers-1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.last_layer = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = []\n",
        "        for _ in range(num_layers):\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.convs = torch.nn.ModuleList(self.convs)\n",
        "        self.bns = torch.nn.ModuleList(self.bns)\n",
        "\n",
        "        # The log softmax layer\n",
        "        self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "\n",
        "        # Probability of an element getting zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "        for i in range(len(self.convs)):\n",
        "            x = self.convs[i](x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.last_layer(x, adj_t)\n",
        "\n",
        "        if not self.return_embeds:\n",
        "          x = self.softmax(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF1hnHUhO81e"
      },
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(data.x, data.adj_t)  # Perform a single forward pass.\n",
        "    loss = loss_fn(out[train_idx], data.y[train_idx].reshape(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJdlrJQhPBsK"
      },
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      df.to_csv(f'{dataset_name}_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7F46xkuLiOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f752c9-744b-4ef2-feee-142b2570745f"
      },
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 200,\n",
        "}\n",
        "args"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'num_layers': 3,\n",
              " 'hidden_dim': 256,\n",
              " 'dropout': 0.5,\n",
              " 'lr': 0.01,\n",
              " 'epochs': 200}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT8RyM2cPGxM"
      },
      "source": [
        "model = GCN(num_features, args['hidden_dim'],\n",
        "            num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name=dataset_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd5O5cnPPdVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608b2c01-bbb6-4f99-8be1-cc31a976d566"
      },
      "source": [
        "import copy\n",
        "\n",
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, sub_graph, split_idx['train'], optimizer, loss_fn)\n",
        "  result = test(model, sub_graph, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-bd2bcc944472>:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.4047, Train: 51.90%, Valid: 51.48% Test: 52.90%\n",
            "Epoch: 02, Loss: 1.9670, Train: 72.06%, Valid: 71.20% Test: 72.46%\n",
            "Epoch: 03, Loss: 1.1687, Train: 76.82%, Valid: 75.78% Test: 77.04%\n",
            "Epoch: 04, Loss: 0.9450, Train: 80.06%, Valid: 79.50% Test: 80.28%\n",
            "Epoch: 05, Loss: 0.8490, Train: 81.72%, Valid: 81.18% Test: 82.19%\n",
            "Epoch: 06, Loss: 0.7868, Train: 83.05%, Valid: 82.70% Test: 83.21%\n",
            "Epoch: 07, Loss: 0.7415, Train: 84.23%, Valid: 83.54% Test: 84.25%\n",
            "Epoch: 08, Loss: 0.6992, Train: 84.76%, Valid: 83.90% Test: 84.46%\n",
            "Epoch: 09, Loss: 0.6666, Train: 84.99%, Valid: 84.42% Test: 84.73%\n",
            "Epoch: 10, Loss: 0.6375, Train: 85.54%, Valid: 84.98% Test: 85.29%\n",
            "Epoch: 11, Loss: 0.6167, Train: 85.93%, Valid: 85.16% Test: 85.77%\n",
            "Epoch: 12, Loss: 0.5949, Train: 86.22%, Valid: 85.36% Test: 86.09%\n",
            "Epoch: 13, Loss: 0.5844, Train: 86.41%, Valid: 85.54% Test: 86.29%\n",
            "Epoch: 14, Loss: 0.5724, Train: 86.57%, Valid: 85.66% Test: 86.51%\n",
            "Epoch: 15, Loss: 0.5616, Train: 86.68%, Valid: 85.94% Test: 86.49%\n",
            "Epoch: 16, Loss: 0.5545, Train: 86.93%, Valid: 86.10% Test: 86.85%\n",
            "Epoch: 17, Loss: 0.5407, Train: 87.14%, Valid: 86.32% Test: 86.97%\n",
            "Epoch: 18, Loss: 0.5360, Train: 87.32%, Valid: 86.54% Test: 86.99%\n",
            "Epoch: 19, Loss: 0.5287, Train: 87.44%, Valid: 86.76% Test: 87.03%\n",
            "Epoch: 20, Loss: 0.5251, Train: 87.50%, Valid: 86.72% Test: 87.11%\n",
            "Epoch: 21, Loss: 0.5198, Train: 87.56%, Valid: 86.88% Test: 87.21%\n",
            "Epoch: 22, Loss: 0.5115, Train: 87.65%, Valid: 87.06% Test: 87.28%\n",
            "Epoch: 23, Loss: 0.5050, Train: 87.72%, Valid: 87.24% Test: 87.27%\n",
            "Epoch: 24, Loss: 0.4954, Train: 87.83%, Valid: 87.36% Test: 87.37%\n",
            "Epoch: 25, Loss: 0.4906, Train: 87.86%, Valid: 87.34% Test: 87.46%\n",
            "Epoch: 26, Loss: 0.4853, Train: 87.96%, Valid: 87.42% Test: 87.50%\n",
            "Epoch: 27, Loss: 0.4813, Train: 88.00%, Valid: 87.38% Test: 87.51%\n",
            "Epoch: 28, Loss: 0.4772, Train: 88.06%, Valid: 87.48% Test: 87.52%\n",
            "Epoch: 29, Loss: 0.4736, Train: 88.15%, Valid: 87.44% Test: 87.55%\n",
            "Epoch: 30, Loss: 0.4700, Train: 88.20%, Valid: 87.56% Test: 87.69%\n",
            "Epoch: 31, Loss: 0.4663, Train: 88.22%, Valid: 87.54% Test: 87.73%\n",
            "Epoch: 32, Loss: 0.4632, Train: 88.31%, Valid: 87.62% Test: 87.75%\n",
            "Epoch: 33, Loss: 0.4585, Train: 88.41%, Valid: 87.70% Test: 87.85%\n",
            "Epoch: 34, Loss: 0.4533, Train: 88.53%, Valid: 87.88% Test: 87.87%\n",
            "Epoch: 35, Loss: 0.4513, Train: 88.70%, Valid: 88.12% Test: 88.05%\n",
            "Epoch: 36, Loss: 0.4479, Train: 88.76%, Valid: 88.30% Test: 88.13%\n",
            "Epoch: 37, Loss: 0.4464, Train: 88.81%, Valid: 88.28% Test: 88.16%\n",
            "Epoch: 38, Loss: 0.4407, Train: 88.82%, Valid: 88.42% Test: 88.13%\n",
            "Epoch: 39, Loss: 0.4381, Train: 88.85%, Valid: 88.66% Test: 88.16%\n",
            "Epoch: 40, Loss: 0.4358, Train: 88.90%, Valid: 88.72% Test: 88.19%\n",
            "Epoch: 41, Loss: 0.4340, Train: 88.94%, Valid: 88.74% Test: 88.33%\n",
            "Epoch: 42, Loss: 0.4328, Train: 89.03%, Valid: 88.74% Test: 88.38%\n",
            "Epoch: 43, Loss: 0.4293, Train: 89.06%, Valid: 88.76% Test: 88.41%\n",
            "Epoch: 44, Loss: 0.4251, Train: 89.12%, Valid: 88.66% Test: 88.47%\n",
            "Epoch: 45, Loss: 0.4235, Train: 89.17%, Valid: 88.68% Test: 88.45%\n",
            "Epoch: 46, Loss: 0.4229, Train: 89.20%, Valid: 88.64% Test: 88.44%\n",
            "Epoch: 47, Loss: 0.4181, Train: 89.20%, Valid: 88.60% Test: 88.55%\n",
            "Epoch: 48, Loss: 0.4178, Train: 89.23%, Valid: 88.68% Test: 88.55%\n",
            "Epoch: 49, Loss: 0.4147, Train: 89.29%, Valid: 88.82% Test: 88.59%\n",
            "Epoch: 50, Loss: 0.4124, Train: 89.37%, Valid: 88.84% Test: 88.63%\n",
            "Epoch: 51, Loss: 0.4117, Train: 89.41%, Valid: 88.82% Test: 88.64%\n",
            "Epoch: 52, Loss: 0.4116, Train: 89.44%, Valid: 88.86% Test: 88.71%\n",
            "Epoch: 53, Loss: 0.4052, Train: 89.48%, Valid: 88.96% Test: 88.75%\n",
            "Epoch: 54, Loss: 0.4055, Train: 89.51%, Valid: 89.00% Test: 88.76%\n",
            "Epoch: 55, Loss: 0.4038, Train: 89.48%, Valid: 89.06% Test: 88.83%\n",
            "Epoch: 56, Loss: 0.4022, Train: 89.52%, Valid: 89.12% Test: 88.89%\n",
            "Epoch: 57, Loss: 0.4020, Train: 89.64%, Valid: 89.26% Test: 88.90%\n",
            "Epoch: 58, Loss: 0.4016, Train: 89.68%, Valid: 89.22% Test: 88.89%\n",
            "Epoch: 59, Loss: 0.3976, Train: 89.67%, Valid: 89.26% Test: 88.89%\n",
            "Epoch: 60, Loss: 0.3953, Train: 89.78%, Valid: 89.28% Test: 88.99%\n",
            "Epoch: 61, Loss: 0.3924, Train: 89.83%, Valid: 89.30% Test: 89.09%\n",
            "Epoch: 62, Loss: 0.3926, Train: 89.88%, Valid: 89.28% Test: 89.09%\n",
            "Epoch: 63, Loss: 0.3924, Train: 89.90%, Valid: 89.38% Test: 89.16%\n",
            "Epoch: 64, Loss: 0.3887, Train: 89.92%, Valid: 89.44% Test: 89.09%\n",
            "Epoch: 65, Loss: 0.3900, Train: 89.95%, Valid: 89.40% Test: 89.07%\n",
            "Epoch: 66, Loss: 0.3857, Train: 90.00%, Valid: 89.50% Test: 89.10%\n",
            "Epoch: 67, Loss: 0.3845, Train: 90.02%, Valid: 89.52% Test: 89.14%\n",
            "Epoch: 68, Loss: 0.3835, Train: 90.03%, Valid: 89.52% Test: 89.25%\n",
            "Epoch: 69, Loss: 0.3827, Train: 90.08%, Valid: 89.42% Test: 89.35%\n",
            "Epoch: 70, Loss: 0.3796, Train: 90.12%, Valid: 89.36% Test: 89.39%\n",
            "Epoch: 71, Loss: 0.3784, Train: 90.17%, Valid: 89.38% Test: 89.40%\n",
            "Epoch: 72, Loss: 0.3767, Train: 90.19%, Valid: 89.60% Test: 89.40%\n",
            "Epoch: 73, Loss: 0.3780, Train: 90.22%, Valid: 89.58% Test: 89.43%\n",
            "Epoch: 74, Loss: 0.3757, Train: 90.22%, Valid: 89.48% Test: 89.45%\n",
            "Epoch: 75, Loss: 0.3755, Train: 90.30%, Valid: 89.64% Test: 89.45%\n",
            "Epoch: 76, Loss: 0.3718, Train: 90.33%, Valid: 89.64% Test: 89.48%\n",
            "Epoch: 77, Loss: 0.3715, Train: 90.36%, Valid: 89.60% Test: 89.52%\n",
            "Epoch: 78, Loss: 0.3714, Train: 90.38%, Valid: 89.54% Test: 89.52%\n",
            "Epoch: 79, Loss: 0.3719, Train: 90.43%, Valid: 89.68% Test: 89.58%\n",
            "Epoch: 80, Loss: 0.3687, Train: 90.42%, Valid: 89.62% Test: 89.52%\n",
            "Epoch: 81, Loss: 0.3698, Train: 90.50%, Valid: 89.56% Test: 89.55%\n",
            "Epoch: 82, Loss: 0.3674, Train: 90.51%, Valid: 89.54% Test: 89.53%\n",
            "Epoch: 83, Loss: 0.3655, Train: 90.53%, Valid: 89.66% Test: 89.53%\n",
            "Epoch: 84, Loss: 0.3662, Train: 90.58%, Valid: 89.78% Test: 89.54%\n",
            "Epoch: 85, Loss: 0.3648, Train: 90.61%, Valid: 89.80% Test: 89.65%\n",
            "Epoch: 86, Loss: 0.3626, Train: 90.59%, Valid: 89.88% Test: 89.63%\n",
            "Epoch: 87, Loss: 0.3614, Train: 90.60%, Valid: 89.74% Test: 89.65%\n",
            "Epoch: 88, Loss: 0.3621, Train: 90.68%, Valid: 89.82% Test: 89.58%\n",
            "Epoch: 89, Loss: 0.3609, Train: 90.59%, Valid: 89.68% Test: 89.55%\n",
            "Epoch: 90, Loss: 0.3600, Train: 90.71%, Valid: 89.92% Test: 89.69%\n",
            "Epoch: 91, Loss: 0.3590, Train: 90.69%, Valid: 90.06% Test: 89.60%\n",
            "Epoch: 92, Loss: 0.3575, Train: 90.66%, Valid: 89.98% Test: 89.55%\n",
            "Epoch: 93, Loss: 0.3580, Train: 90.74%, Valid: 89.92% Test: 89.68%\n",
            "Epoch: 94, Loss: 0.3570, Train: 90.75%, Valid: 89.80% Test: 89.75%\n",
            "Epoch: 95, Loss: 0.3557, Train: 90.79%, Valid: 89.84% Test: 89.74%\n",
            "Epoch: 96, Loss: 0.3527, Train: 90.81%, Valid: 89.92% Test: 89.73%\n",
            "Epoch: 97, Loss: 0.3512, Train: 90.85%, Valid: 89.76% Test: 89.85%\n",
            "Epoch: 98, Loss: 0.3511, Train: 90.86%, Valid: 89.74% Test: 89.71%\n",
            "Epoch: 99, Loss: 0.3504, Train: 90.92%, Valid: 89.90% Test: 89.84%\n",
            "Epoch: 100, Loss: 0.3500, Train: 90.89%, Valid: 89.96% Test: 89.78%\n",
            "Epoch: 101, Loss: 0.3493, Train: 90.92%, Valid: 89.84% Test: 89.83%\n",
            "Epoch: 102, Loss: 0.3483, Train: 90.97%, Valid: 89.96% Test: 89.99%\n",
            "Epoch: 103, Loss: 0.3465, Train: 91.01%, Valid: 90.08% Test: 90.07%\n",
            "Epoch: 104, Loss: 0.3469, Train: 90.94%, Valid: 90.00% Test: 90.01%\n",
            "Epoch: 105, Loss: 0.3444, Train: 90.97%, Valid: 90.04% Test: 89.99%\n",
            "Epoch: 106, Loss: 0.3442, Train: 90.96%, Valid: 90.08% Test: 89.84%\n",
            "Epoch: 107, Loss: 0.3440, Train: 91.07%, Valid: 90.12% Test: 89.95%\n",
            "Epoch: 108, Loss: 0.3448, Train: 91.00%, Valid: 90.00% Test: 89.95%\n",
            "Epoch: 109, Loss: 0.3419, Train: 91.04%, Valid: 90.14% Test: 90.02%\n",
            "Epoch: 110, Loss: 0.3419, Train: 90.98%, Valid: 89.84% Test: 89.92%\n",
            "Epoch: 111, Loss: 0.3422, Train: 91.14%, Valid: 90.00% Test: 90.09%\n",
            "Epoch: 112, Loss: 0.3395, Train: 91.21%, Valid: 90.12% Test: 90.05%\n",
            "Epoch: 113, Loss: 0.3386, Train: 91.12%, Valid: 89.96% Test: 89.95%\n",
            "Epoch: 114, Loss: 0.3408, Train: 91.11%, Valid: 90.04% Test: 89.93%\n",
            "Epoch: 115, Loss: 0.3371, Train: 91.21%, Valid: 90.06% Test: 89.97%\n",
            "Epoch: 116, Loss: 0.3386, Train: 91.20%, Valid: 89.86% Test: 90.03%\n",
            "Epoch: 117, Loss: 0.3371, Train: 91.19%, Valid: 90.16% Test: 90.06%\n",
            "Epoch: 118, Loss: 0.3360, Train: 91.14%, Valid: 90.20% Test: 89.99%\n",
            "Epoch: 119, Loss: 0.3360, Train: 91.17%, Valid: 90.36% Test: 90.01%\n",
            "Epoch: 120, Loss: 0.3340, Train: 91.23%, Valid: 90.36% Test: 90.05%\n",
            "Epoch: 121, Loss: 0.3347, Train: 91.24%, Valid: 90.38% Test: 89.89%\n",
            "Epoch: 122, Loss: 0.3331, Train: 91.24%, Valid: 90.32% Test: 89.98%\n",
            "Epoch: 123, Loss: 0.3328, Train: 91.28%, Valid: 90.38% Test: 90.09%\n",
            "Epoch: 124, Loss: 0.3316, Train: 91.29%, Valid: 90.46% Test: 90.05%\n",
            "Epoch: 125, Loss: 0.3331, Train: 91.34%, Valid: 90.20% Test: 90.10%\n",
            "Epoch: 126, Loss: 0.3304, Train: 91.29%, Valid: 90.32% Test: 90.05%\n",
            "Epoch: 127, Loss: 0.3335, Train: 91.34%, Valid: 90.26% Test: 90.06%\n",
            "Epoch: 128, Loss: 0.3289, Train: 91.26%, Valid: 90.04% Test: 90.01%\n",
            "Epoch: 129, Loss: 0.3310, Train: 91.34%, Valid: 90.30% Test: 90.08%\n",
            "Epoch: 130, Loss: 0.3292, Train: 91.37%, Valid: 90.40% Test: 90.12%\n",
            "Epoch: 131, Loss: 0.3303, Train: 91.42%, Valid: 90.48% Test: 90.25%\n",
            "Epoch: 132, Loss: 0.3261, Train: 91.34%, Valid: 90.18% Test: 90.05%\n",
            "Epoch: 133, Loss: 0.3248, Train: 91.37%, Valid: 89.94% Test: 90.11%\n",
            "Epoch: 134, Loss: 0.3277, Train: 91.37%, Valid: 90.26% Test: 90.03%\n",
            "Epoch: 135, Loss: 0.3242, Train: 91.46%, Valid: 90.38% Test: 90.19%\n",
            "Epoch: 136, Loss: 0.3260, Train: 91.30%, Valid: 89.92% Test: 90.07%\n",
            "Epoch: 137, Loss: 0.3263, Train: 91.37%, Valid: 90.36% Test: 89.93%\n",
            "Epoch: 138, Loss: 0.3257, Train: 91.43%, Valid: 90.42% Test: 90.05%\n",
            "Epoch: 139, Loss: 0.3216, Train: 91.36%, Valid: 90.10% Test: 89.98%\n",
            "Epoch: 140, Loss: 0.3244, Train: 91.41%, Valid: 90.50% Test: 90.21%\n",
            "Epoch: 141, Loss: 0.3236, Train: 91.49%, Valid: 90.28% Test: 90.25%\n",
            "Epoch: 142, Loss: 0.3211, Train: 91.54%, Valid: 90.32% Test: 90.26%\n",
            "Epoch: 143, Loss: 0.3233, Train: 91.59%, Valid: 90.32% Test: 90.34%\n",
            "Epoch: 144, Loss: 0.3203, Train: 91.61%, Valid: 90.50% Test: 90.23%\n",
            "Epoch: 145, Loss: 0.3202, Train: 91.62%, Valid: 90.44% Test: 90.17%\n",
            "Epoch: 146, Loss: 0.3189, Train: 91.55%, Valid: 90.42% Test: 90.13%\n",
            "Epoch: 147, Loss: 0.3182, Train: 91.56%, Valid: 90.42% Test: 90.17%\n",
            "Epoch: 148, Loss: 0.3181, Train: 91.65%, Valid: 90.24% Test: 90.33%\n",
            "Epoch: 149, Loss: 0.3184, Train: 91.59%, Valid: 90.54% Test: 90.16%\n",
            "Epoch: 150, Loss: 0.3170, Train: 91.67%, Valid: 90.36% Test: 90.24%\n",
            "Epoch: 151, Loss: 0.3178, Train: 91.70%, Valid: 90.38% Test: 90.22%\n",
            "Epoch: 152, Loss: 0.3160, Train: 91.70%, Valid: 90.64% Test: 90.23%\n",
            "Epoch: 153, Loss: 0.3160, Train: 91.51%, Valid: 90.50% Test: 90.24%\n",
            "Epoch: 154, Loss: 0.3180, Train: 91.46%, Valid: 89.96% Test: 90.32%\n",
            "Epoch: 155, Loss: 0.3182, Train: 91.60%, Valid: 90.34% Test: 90.34%\n",
            "Epoch: 156, Loss: 0.3171, Train: 91.59%, Valid: 90.20% Test: 90.26%\n",
            "Epoch: 157, Loss: 0.3162, Train: 91.50%, Valid: 90.00% Test: 90.20%\n",
            "Epoch: 158, Loss: 0.3126, Train: 91.61%, Valid: 90.32% Test: 90.24%\n",
            "Epoch: 159, Loss: 0.3146, Train: 91.66%, Valid: 90.44% Test: 90.26%\n",
            "Epoch: 160, Loss: 0.3131, Train: 91.52%, Valid: 90.20% Test: 90.18%\n",
            "Epoch: 161, Loss: 0.3132, Train: 91.77%, Valid: 90.54% Test: 90.49%\n",
            "Epoch: 162, Loss: 0.3119, Train: 91.80%, Valid: 90.40% Test: 90.58%\n",
            "Epoch: 163, Loss: 0.3125, Train: 91.67%, Valid: 90.10% Test: 90.48%\n",
            "Epoch: 164, Loss: 0.3137, Train: 91.73%, Valid: 90.32% Test: 90.45%\n",
            "Epoch: 165, Loss: 0.3105, Train: 91.73%, Valid: 90.38% Test: 90.31%\n",
            "Epoch: 166, Loss: 0.3122, Train: 91.70%, Valid: 90.22% Test: 90.33%\n",
            "Epoch: 167, Loss: 0.3111, Train: 91.77%, Valid: 90.60% Test: 90.38%\n",
            "Epoch: 168, Loss: 0.3104, Train: 91.75%, Valid: 90.50% Test: 90.29%\n",
            "Epoch: 169, Loss: 0.3106, Train: 91.78%, Valid: 90.20% Test: 90.40%\n",
            "Epoch: 170, Loss: 0.3082, Train: 91.89%, Valid: 90.66% Test: 90.57%\n",
            "Epoch: 171, Loss: 0.3080, Train: 91.77%, Valid: 90.30% Test: 90.41%\n",
            "Epoch: 172, Loss: 0.3100, Train: 91.81%, Valid: 90.38% Test: 90.35%\n",
            "Epoch: 173, Loss: 0.3075, Train: 91.89%, Valid: 90.38% Test: 90.43%\n",
            "Epoch: 174, Loss: 0.3066, Train: 91.83%, Valid: 90.50% Test: 90.29%\n",
            "Epoch: 175, Loss: 0.3074, Train: 91.88%, Valid: 90.62% Test: 90.37%\n",
            "Epoch: 176, Loss: 0.3069, Train: 91.86%, Valid: 90.44% Test: 90.31%\n",
            "Epoch: 177, Loss: 0.3067, Train: 91.90%, Valid: 90.64% Test: 90.37%\n",
            "Epoch: 178, Loss: 0.3057, Train: 91.88%, Valid: 90.56% Test: 90.37%\n",
            "Epoch: 179, Loss: 0.3044, Train: 91.89%, Valid: 90.20% Test: 90.43%\n",
            "Epoch: 180, Loss: 0.3039, Train: 91.93%, Valid: 90.48% Test: 90.43%\n",
            "Epoch: 181, Loss: 0.3016, Train: 92.01%, Valid: 90.54% Test: 90.58%\n",
            "Epoch: 182, Loss: 0.3030, Train: 91.97%, Valid: 90.42% Test: 90.49%\n",
            "Epoch: 183, Loss: 0.3030, Train: 91.96%, Valid: 90.56% Test: 90.53%\n",
            "Epoch: 184, Loss: 0.3006, Train: 91.99%, Valid: 90.68% Test: 90.55%\n",
            "Epoch: 185, Loss: 0.3019, Train: 92.01%, Valid: 90.72% Test: 90.58%\n",
            "Epoch: 186, Loss: 0.3004, Train: 92.01%, Valid: 90.72% Test: 90.59%\n",
            "Epoch: 187, Loss: 0.3007, Train: 92.09%, Valid: 90.78% Test: 90.62%\n",
            "Epoch: 188, Loss: 0.2990, Train: 92.06%, Valid: 90.52% Test: 90.57%\n",
            "Epoch: 189, Loss: 0.2982, Train: 92.07%, Valid: 90.56% Test: 90.66%\n",
            "Epoch: 190, Loss: 0.3000, Train: 91.98%, Valid: 90.24% Test: 90.49%\n",
            "Epoch: 191, Loss: 0.3008, Train: 91.97%, Valid: 90.66% Test: 90.47%\n",
            "Epoch: 192, Loss: 0.3026, Train: 91.73%, Valid: 90.14% Test: 90.27%\n",
            "Epoch: 193, Loss: 0.3041, Train: 92.14%, Valid: 90.50% Test: 90.48%\n",
            "Epoch: 194, Loss: 0.2983, Train: 92.03%, Valid: 90.48% Test: 90.43%\n",
            "Epoch: 195, Loss: 0.3002, Train: 91.90%, Valid: 90.20% Test: 90.38%\n",
            "Epoch: 196, Loss: 0.3027, Train: 92.03%, Valid: 90.52% Test: 90.49%\n",
            "Epoch: 197, Loss: 0.2992, Train: 92.06%, Valid: 90.46% Test: 90.53%\n",
            "Epoch: 198, Loss: 0.2998, Train: 92.03%, Valid: 90.18% Test: 90.52%\n",
            "Epoch: 199, Loss: 0.2985, Train: 92.03%, Valid: 90.34% Test: 90.65%\n",
            "Epoch: 200, Loss: 0.2956, Train: 91.83%, Valid: 90.22% Test: 90.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcextqOL2FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839e55ba-6d2f-4882-ffa1-bb806bdd3b37"
      },
      "source": [
        "best_result = test(best_model, sub_graph, split_idx, evaluator, save_model_results=True)\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model Predictions\n",
            "Best model: Train: 92.09%, Valid: 90.78% Test: 90.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-bd2bcc944472>:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSMVBeqnFsZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}